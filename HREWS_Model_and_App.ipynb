{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Health Risk Early Warning System (HREWS)\n",
        "## Model Training and Application\n",
        "\n",
        "This notebook contains the complete code from `hrews_model.py` and `app.py` for training the model and running the application.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install pandas>=2.0.0 numpy>=1.24.0 scikit-learn>=1.3.0 xgboost>=2.0.0\n",
        "%pip install matplotlib>=3.7.0 seaborn>=0.12.0 plotly>=5.17.0\n",
        "%pip install joblib>=1.3.0 scipy>=1.11.0\n",
        "%pip install shap>=0.42.1 lime>=0.2.0.1\n",
        "%pip install streamlit>=1.28.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optional explainability libraries\n",
        "try:\n",
        "    import shap\n",
        "except Exception:\n",
        "    shap = None\n",
        "\n",
        "try:\n",
        "    from lime.lime_tabular import LimeTabularExplainer\n",
        "except Exception:\n",
        "    LimeTabularExplainer = None\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HealthRiskPredictor Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HealthRiskPredictor:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.best_model = None\n",
        "        self.feature_names = None\n",
        "        \n",
        "    def load_data(self, file_path):\n",
        "        \"\"\"Load and preprocess the health risk dataset\"\"\"\n",
        "        print(\"Loading dataset...\")\n",
        "        self.data = pd.read_csv(file_path)\n",
        "        print(f\"Dataset loaded: {self.data.shape[0]} patients, {self.data.shape[1]} features\")\n",
        "        return self.data\n",
        "    \n",
        "    def preprocess_data(self):\n",
        "        \"\"\"Preprocess the data for machine learning\"\"\"\n",
        "        print(\"Preprocessing data...\")\n",
        "        \n",
        "        # Create a copy for preprocessing\n",
        "        df = self.data.copy()\n",
        "        \n",
        "        # Handle categorical variables - convert to numeric for XGBoost compatibility\n",
        "        df['Consciousness'] = df['Consciousness'].astype('category')\n",
        "        df['On_Oxygen'] = df['On_Oxygen'].astype(int)  # Convert to int instead of category\n",
        "        \n",
        "        # One-hot encode consciousness\n",
        "        consciousness_dummies = pd.get_dummies(df['Consciousness'], prefix='Consciousness')\n",
        "        df = pd.concat([df, consciousness_dummies], axis=1)\n",
        "        df.drop('Consciousness', axis=1, inplace=True)\n",
        "        \n",
        "        # Encode target variable\n",
        "        df['Risk_Level_Encoded'] = self.label_encoder.fit_transform(df['Risk_Level'])\n",
        "        \n",
        "        # Select features for modeling\n",
        "        feature_columns = ['Respiratory_Rate', 'Oxygen_Saturation', 'O2_Scale', \n",
        "                          'Systolic_BP', 'Heart_Rate', 'Temperature', 'On_Oxygen'] + \\\n",
        "                         [col for col in df.columns if col.startswith('Consciousness_')]\n",
        "        \n",
        "        self.feature_names = feature_columns\n",
        "        X = df[feature_columns]\n",
        "        y = df['Risk_Level_Encoded']\n",
        "        \n",
        "        # Split the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "        \n",
        "        # Scale the features\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "        \n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        self.X_train_scaled = X_train_scaled\n",
        "        self.X_test_scaled = X_test_scaled\n",
        "        \n",
        "        print(f\"Data preprocessed. Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "        print(f\"Feature names: {self.feature_names}\")\n",
        "        \n",
        "        return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "    \n",
        "    def train_models(self):\n",
        "        \"\"\"Train multiple machine learning models\"\"\"\n",
        "        print(\"Training models...\")\n",
        "        \n",
        "        # Initialize models\n",
        "        self.models = {\n",
        "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "            'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "            'SVM': SVC(random_state=42, probability=True),\n",
        "            'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
        "        }\n",
        "        \n",
        "        # Train and evaluate each model\n",
        "        results = {}\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"Training {name}...\")\n",
        "            \n",
        "            if name in ['Logistic Regression', 'SVM']:\n",
        "                model.fit(self.X_train_scaled, self.y_train)\n",
        "                y_pred = model.predict(self.X_test_scaled)\n",
        "                y_pred_proba = model.predict_proba(self.X_test_scaled) if hasattr(model, 'predict_proba') else None\n",
        "            else:\n",
        "                model.fit(self.X_train, self.y_train)\n",
        "                y_pred = model.predict(self.X_test)\n",
        "                y_pred_proba = model.predict_proba(self.X_test)\n",
        "            \n",
        "            # Calculate metrics\n",
        "            accuracy = accuracy_score(self.y_test, y_pred)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(self.y_test, y_pred, average='weighted')\n",
        "            \n",
        "            results[name] = {\n",
        "                'model': model,\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'predictions': y_pred,\n",
        "                'probabilities': y_pred_proba\n",
        "            }\n",
        "            \n",
        "            print(f\"{name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
        "        \n",
        "        # Find best model\n",
        "        best_model_name = max(results.keys(), key=lambda k: results[k]['f1'])\n",
        "        self.best_model = results[best_model_name]['model']\n",
        "        \n",
        "        print(f\"\\nBest model: {best_model_name}\")\n",
        "        print(f\"Best F1 Score: {results[best_model_name]['f1']:.4f}\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def evaluate_best_model(self):\n",
        "        \"\"\"Evaluate the best performing model in detail\"\"\"\n",
        "        if self.best_model is None:\n",
        "            print(\"No best model selected. Please train models first.\")\n",
        "            return\n",
        "        \n",
        "        print(\"\\nDetailed evaluation of best model:\")\n",
        "        \n",
        "        # Make predictions\n",
        "        if isinstance(self.best_model, (LogisticRegression, SVC)):\n",
        "            y_pred = self.best_model.predict(self.X_test_scaled)\n",
        "            y_pred_proba = self.best_model.predict_proba(self.X_test_scaled) if hasattr(self.best_model, 'predict_proba') else None\n",
        "        else:\n",
        "            y_pred = self.best_model.predict(self.X_test)\n",
        "            y_pred_proba = self.best_model.predict_proba(self.X_test)\n",
        "        \n",
        "        # Classification report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(self.y_test, y_pred, \n",
        "                                  target_names=self.label_encoder.classes_))\n",
        "        \n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(self.y_test, y_pred)\n",
        "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
        "        \n",
        "        return y_pred, y_pred_proba\n",
        "    \n",
        "    def explain_shap(self, data_row=None, nsamples=100):\n",
        "        \"\"\"Return SHAP explanations for a single data row.\n",
        "\n",
        "        Returns a dict mapping feature name -> SHAP value for the predicted class.\n",
        "        \"\"\"\n",
        "        if shap is None:\n",
        "            raise ImportError(\"shap is not installed. Install with `pip install shap`\")\n",
        "\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"No trained model available. Train or load a model first.\")\n",
        "\n",
        "        # Choose background dataset (use a small sample for speed)\n",
        "        try:\n",
        "            background = (self.X_train if hasattr(self, 'X_train') else None)\n",
        "        except Exception:\n",
        "            background = None\n",
        "\n",
        "        if data_row is None:\n",
        "            # Use first test row by default\n",
        "            if hasattr(self, 'X_test') and len(self.X_test) > 0:\n",
        "                instance = self.X_test.iloc[0].values.reshape(1, -1)\n",
        "            elif hasattr(self, 'X_test_scaled') and len(self.X_test_scaled) > 0:\n",
        "                instance = self.X_test_scaled[0].reshape(1, -1)\n",
        "            else:\n",
        "                raise ValueError(\"No data available to explain. Provide `data_row`.\")\n",
        "        else:\n",
        "            # Accept either dict of feature->value or array-like\n",
        "            if isinstance(data_row, dict):\n",
        "                instance = np.array([data_row[f] for f in self.feature_names], dtype=float).reshape(1, -1)\n",
        "            else:\n",
        "                instance = np.array(data_row, dtype=float).reshape(1, -1)\n",
        "\n",
        "        # Decide whether model expects scaled input\n",
        "        use_scaled = isinstance(self.best_model, (LogisticRegression, SVC))\n",
        "        if use_scaled:\n",
        "            background_data = (self.X_train_scaled if hasattr(self, 'X_train_scaled') else background)\n",
        "            instance_for_expl = (self.scaler.transform(instance) if hasattr(self, 'scaler') else instance)\n",
        "        else:\n",
        "            background_data = (self.X_train if hasattr(self, 'X_train') else None)\n",
        "            instance_for_expl = instance\n",
        "\n",
        "        # Fallback: sample small background for KernelExplainer if needed\n",
        "        if background_data is None:\n",
        "            raise ValueError(\"No background data available for SHAP explanation\")\n",
        "\n",
        "        # Use the new shap.Explainer which adapts to model type (Tree/Linear/Kernel)\n",
        "        try:\n",
        "            bg_sample = background_data if isinstance(background_data, np.ndarray) else np.array(background_data[:min(100, len(background_data))])\n",
        "            # Use LinearExplainer for linear models for more consistent output\n",
        "            if isinstance(self.best_model, LogisticRegression) or hasattr(self.best_model, 'coef_'):\n",
        "                explainer = shap.LinearExplainer(self.best_model, bg_sample, feature_perturbation='interventional')\n",
        "                shap_exp = explainer(instance_for_expl)\n",
        "            else:\n",
        "                explainer = shap.Explainer(self.best_model, bg_sample)\n",
        "                shap_exp = explainer(instance_for_expl)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to compute SHAP values: {e}\")\n",
        "\n",
        "        # shap_exp may contain .values with different shapes depending on model/multiclass\n",
        "        try:\n",
        "            vals = getattr(shap_exp, 'values', None)\n",
        "            if vals is None:\n",
        "                # fallback to shap_values being a list\n",
        "                shap_values = shap_exp\n",
        "                if isinstance(shap_values, list):\n",
        "                    pred = np.argmax(self.best_model.predict_proba(instance_for_expl)[0])\n",
        "                    values = shap_values[pred][0]\n",
        "                else:\n",
        "                    values = np.array(shap_values).flatten()\n",
        "            else:\n",
        "                vals = np.array(vals)\n",
        "                # Possible shapes:\n",
        "                #  - (n_instances, n_features)\n",
        "                #  - (n_classes, n_instances, n_features)\n",
        "                #  - (n_instances, n_features, n_classes)\n",
        "                pred = np.argmax(self.best_model.predict_proba(instance_for_expl)[0])\n",
        "                if vals.ndim == 3:\n",
        "                    # try detect class axis\n",
        "                    n_classes = len(self.label_encoder.classes_) if hasattr(self, 'label_encoder') else None\n",
        "                    # case: (n_classes, n_instances, n_features)\n",
        "                    if n_classes is not None and vals.shape[0] == n_classes:\n",
        "                        values = vals[pred][0]\n",
        "                    # case: (n_instances, n_features, n_classes)\n",
        "                    elif n_classes is not None and vals.shape[2] == n_classes:\n",
        "                        values = vals[0, :, pred]\n",
        "                    else:\n",
        "                        # fallback: pick first instance and try to reduce\n",
        "                        try:\n",
        "                            values = vals[0].flatten()\n",
        "                        except Exception:\n",
        "                            values = vals.flatten()\n",
        "                elif vals.ndim == 2:\n",
        "                    # (n_instances, n_features)\n",
        "                    values = vals[0]\n",
        "                else:\n",
        "                    values = vals.flatten()\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to parse SHAP values: {e}\")\n",
        "\n",
        "        return dict(zip(self.feature_names, [float(v) for v in values]))\n",
        "\n",
        "    def explain_lime(self, data_row=None, num_features=10):\n",
        "        \"\"\"Return LIME explanations (list of (feature, contribution)).\"\"\"\n",
        "        if LimeTabularExplainer is None:\n",
        "            raise ImportError(\"lime is not installed. Install with `pip install lime`\")\n",
        "\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"No trained model available. Train or load a model first.\")\n",
        "\n",
        "        if data_row is None:\n",
        "            if hasattr(self, 'X_test') and len(self.X_test) > 0:\n",
        "                instance = self.X_test.iloc[0].values\n",
        "            elif hasattr(self, 'X_test_scaled') and len(self.X_test_scaled) > 0:\n",
        "                instance = self.X_test_scaled[0]\n",
        "            else:\n",
        "                raise ValueError(\"No data available to explain. Provide `data_row`.\")\n",
        "        else:\n",
        "            if isinstance(data_row, dict):\n",
        "                instance = np.array([data_row[f] for f in self.feature_names], dtype=float)\n",
        "            else:\n",
        "                instance = np.array(data_row, dtype=float)\n",
        "\n",
        "        # LIME explanation expects the training data used for the model\n",
        "        use_scaled = isinstance(self.best_model, (LogisticRegression, SVC))\n",
        "        train_data = (self.X_train_scaled if use_scaled and hasattr(self, 'X_train_scaled') else self.X_train)\n",
        "        if train_data is None:\n",
        "            raise ValueError(\"No training data available for LIME explainer\")\n",
        "\n",
        "        explainer = LimeTabularExplainer(\n",
        "            training_data=np.array(train_data),\n",
        "            feature_names=self.feature_names,\n",
        "            class_names=list(self.label_encoder.classes_),\n",
        "            discretize_continuous=True\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            exp = explainer.explain_instance(instance, self.best_model.predict_proba, num_features=num_features)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to compute LIME explanation: {e}\")\n",
        "\n",
        "        # Return list of (feature, contribution) for the top features\n",
        "        return exp.as_list()\n",
        "    \n",
        "    def predict_risk(self, patient_data):\n",
        "        \"\"\"Predict risk level for new patient data\"\"\"\n",
        "        if self.best_model is None:\n",
        "            print(\"No trained model available.\")\n",
        "            return None\n",
        "        \n",
        "        # Preprocess patient data\n",
        "        processed_data = self.preprocess_patient_data(patient_data)\n",
        "        \n",
        "        # Make prediction\n",
        "        if isinstance(self.best_model, (LogisticRegression, SVC)):\n",
        "            risk_proba = self.best_model.predict_proba(processed_data.reshape(1, -1))[0]\n",
        "        else:\n",
        "            risk_proba = self.best_model.predict_proba(processed_data.reshape(1, -1))[0]\n",
        "        \n",
        "        risk_level = self.best_model.predict(processed_data.reshape(1, -1))[0]\n",
        "        risk_level_name = self.label_encoder.inverse_transform([risk_level])[0]\n",
        "        \n",
        "        # Create risk breakdown\n",
        "        risk_breakdown = {\n",
        "            'predicted_risk': risk_level_name,\n",
        "            'probabilities': dict(zip(self.label_encoder.classes_, risk_proba)),\n",
        "            'escalation_probability': self.calculate_escalation_probability(risk_proba)\n",
        "        }\n",
        "        \n",
        "        return risk_breakdown\n",
        "    \n",
        "    def preprocess_patient_data(self, patient_data):\n",
        "        \"\"\"Preprocess single patient data for prediction\"\"\"\n",
        "        # Extract features in the same order as training\n",
        "        features = []\n",
        "        \n",
        "        # Continuous features\n",
        "        features.extend([\n",
        "            float(patient_data['Respiratory_Rate']),\n",
        "            float(patient_data['Oxygen_Saturation']),\n",
        "            float(patient_data['O2_Scale']),\n",
        "            float(patient_data['Systolic_BP']),\n",
        "            float(patient_data['Heart_Rate']),\n",
        "            float(patient_data['Temperature']),\n",
        "            int(patient_data['On_Oxygen'])  # Ensure it's an integer\n",
        "        ])\n",
        "        \n",
        "        # Consciousness one-hot encoding\n",
        "        consciousness = patient_data['Consciousness']\n",
        "        for level in ['A', 'P', 'C', 'V', 'U']:\n",
        "            features.append(1 if consciousness == level else 0)\n",
        "        \n",
        "        features = np.array(features, dtype=float)\n",
        "        \n",
        "        # Scale if using scaled models\n",
        "        if isinstance(self.best_model, (LogisticRegression, SVC)):\n",
        "            features = self.scaler.transform(features.reshape(1, -1)).flatten()\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def calculate_escalation_probability(self, risk_proba):\n",
        "        \"\"\"Calculate probability of escalation from current risk level\"\"\"\n",
        "        # This is a simplified calculation - in practice, you might want more sophisticated logic\n",
        "        normal_prob = risk_proba[0] if len(risk_proba) > 0 else 0\n",
        "        low_prob = risk_proba[1] if len(risk_proba) > 1 else 0\n",
        "        medium_prob = risk_proba[2] if len(risk_proba) > 2 else 0\n",
        "        high_prob = risk_proba[3] if len(risk_proba) > 3 else 0\n",
        "        \n",
        "        # Probability of being at medium or high risk\n",
        "        escalation_prob = medium_prob + high_prob\n",
        "        \n",
        "        return {\n",
        "            'normal_to_high': normal_prob * high_prob,\n",
        "            'low_to_high': low_prob * high_prob,\n",
        "            'medium_to_high': medium_prob * high_prob,\n",
        "            'overall_escalation': escalation_prob\n",
        "        }\n",
        "    \n",
        "    def get_feature_importance(self):\n",
        "        \"\"\"Get feature importance from the best model\"\"\"\n",
        "        if self.best_model is None:\n",
        "            return None\n",
        "        \n",
        "        if hasattr(self.best_model, 'feature_importances_'):\n",
        "            importance = self.best_model.feature_importances_\n",
        "        elif hasattr(self.best_model, 'coef_'):\n",
        "            importance = np.abs(self.best_model.coef_[0])\n",
        "        else:\n",
        "            return None\n",
        "        \n",
        "        feature_importance = dict(zip(self.feature_names, importance))\n",
        "        return dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))\n",
        "    \n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"Save the trained model and preprocessing objects\"\"\"\n",
        "        model_data = {\n",
        "            'best_model': self.best_model,\n",
        "            'scaler': self.scaler,\n",
        "            'label_encoder': self.label_encoder,\n",
        "            'feature_names': self.feature_names\n",
        "        }\n",
        "        joblib.dump(model_data, filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "    \n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"Load a previously trained model\"\"\"\n",
        "        model_data = joblib.load(filepath)\n",
        "        self.best_model = model_data['best_model']\n",
        "        self.scaler = model_data['scaler']\n",
        "        self.label_encoder = model_data['label_encoder']\n",
        "        self.feature_names = model_data['feature_names']\n",
        "        print(f\"Model loaded from {filepath}\")\n",
        "\n",
        "print(\"HealthRiskPredictor class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the predictor\n",
        "predictor = HealthRiskPredictor()\n",
        "\n",
        "# Load data (upload Health_Risk_Dataset.csv to Colab first)\n",
        "# For Colab: Go to Files -> Upload and upload the CSV file\n",
        "data = predictor.load_data('Health_Risk_Dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess data\n",
        "predictor.preprocess_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train models\n",
        "results = predictor.train_models()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate best model\n",
        "predictor.evaluate_best_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance\n",
        "importance = predictor.get_feature_importance()\n",
        "if importance:\n",
        "    print(\"\\nFeature Importance:\")\n",
        "    for feature, imp in list(importance.items())[:10]:\n",
        "        print(f\"{feature}: {imp:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "predictor.save_model('hrews_model.pkl')\n",
        "\n",
        "print(\"\\nModel training and evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streamlit Application Code\n",
        "\n",
        "**Note:** To run Streamlit in Google Colab, you need to use `ngrok` or `localtunnel`. The code below is the complete Streamlit app code from `app.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streamlit app imports\n",
        "import streamlit as st\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Streamlit caching for expensive resources (model/data)\n",
        "\n",
        "@st.cache_resource\n",
        "def cached_load_model(path: str):\n",
        "    \"\"\"Load and cache a trained HealthRiskPredictor from a pickle file.\"\"\"\n",
        "    try:\n",
        "        if os.path.exists(path):\n",
        "            predictor = HealthRiskPredictor()\n",
        "            predictor.load_model(path)\n",
        "            return predictor\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "def cached_load_data(path: str):\n",
        "    \"\"\"Load and cache dataset CSV.\"\"\"\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "\n",
        "def safe_rerun():\n",
        "    \"\"\"Try to trigger a Streamlit rerun; fallback to setting a query param or asking user to refresh.\"\"\"\n",
        "    try:\n",
        "        # Preferred: direct rerun\n",
        "        if hasattr(st, 'experimental_rerun'):\n",
        "            st.experimental_rerun()\n",
        "            return\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        # Fallback: tweak query params to force rerun by assigning to query_params\n",
        "        st.query_params = {**st.query_params, '_refresh': int(time.time())}\n",
        "        return\n",
        "    except Exception:\n",
        "        try:\n",
        "            st.experimental_set_query_params(_refresh=int(time.time()))\n",
        "            return\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Last resort: ask user to refresh the browser\n",
        "    st.info('Metrics updated. Please refresh the page to see the latest results.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"Health Risk Early Warning System (HREWS)\",\n",
        "    page_icon=\"üöë\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Custom CSS for better styling\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {\n",
        "        font-size: 3rem;\n",
        "        color: #1f77b4;\n",
        "        text-align: center;\n",
        "        margin-bottom: 2rem;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    .risk-high { color: #d62728; font-weight: bold; }\n",
        "    .risk-medium { color: #ff7f0e; font-weight: bold; }\n",
        "    .risk-low { color: #2ca02c; font-weight: bold; }\n",
        "    .risk-normal { color: #1f77b4; font-weight: bold; }\n",
        "    .metric-card {\n",
        "        background-color: #f0f2f6;\n",
        "        padding: 1rem;\n",
        "        border-radius: 0.5rem;\n",
        "        border-left: 4px solid #1f77b4;\n",
        "    }\n",
        "    .sidebar .sidebar-content {\n",
        "        background-color: #f8f9fa;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HREWSApp:\n",
        "    def __init__(self):\n",
        "        self.predictor = None\n",
        "        self.data = None\n",
        "        self.patient_history = []\n",
        "        \n",
        "    def load_model(self):\n",
        "        \"\"\"Load the trained model\"\"\"\n",
        "        try:\n",
        "            predictor = cached_load_model('hrews_model.pkl')\n",
        "            if predictor is not None:\n",
        "                self.predictor = predictor\n",
        "                return True\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error loading model: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def load_data(self):\n",
        "        \"\"\"Load the dataset\"\"\"\n",
        "        try:\n",
        "            self.data = cached_load_data('Health_Risk_Dataset.csv')\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error loading dataset: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def run(self):\n",
        "        \"\"\"Main application runner\"\"\"\n",
        "        # Header\n",
        "        st.markdown('<h1 class=\"main-header\">üöë Health Risk Early Warning System (HREWS)</h1>', unsafe_allow_html=True)\n",
        "        \n",
        "        # Load model and data once (cached)\n",
        "        model_ok = self.load_model()\n",
        "        data_ok = self.load_data()\n",
        "\n",
        "        # Sidebar (uses loaded resources)\n",
        "        self.setup_sidebar()\n",
        "\n",
        "        # Main content checks\n",
        "        if not model_ok:\n",
        "            st.error(\"‚ö†Ô∏è Model not found. Please train the model first by running 'python hrews_model.py'\")\n",
        "            st.info(\"The model training will create 'hrews_model.pkl' file.\")\n",
        "            return\n",
        "\n",
        "        if not data_ok:\n",
        "            st.error(\"‚ö†Ô∏è Dataset not found. Please ensure 'Health_Risk_Dataset.csv' is in the current directory.\")\n",
        "            return\n",
        "        \n",
        "        # Navigation\n",
        "        page_options = [\"üè† Dashboard\", \"üìä Data Analysis\", \"üîÆ Risk Prediction\", \"üìã Model Performance\"]\n",
        "\n",
        "        # Check query params (and other fallbacks) to allow Quick Actions to change the active page\n",
        "        nav = None\n",
        "        try:\n",
        "            # st.query_params may be a dict of lists (Streamlit >=1.10) or similar\n",
        "            qp = st.query_params if hasattr(st, 'query_params') else {}\n",
        "            if qp:\n",
        "                candidate = qp.get('nav')\n",
        "                if candidate:\n",
        "                    # candidate can be list-like or a single string\n",
        "                    nav = candidate[0] if isinstance(candidate, (list, tuple)) else candidate\n",
        "        except Exception:\n",
        "            nav = None\n",
        "\n",
        "        # Map short nav keys to sidebar labels\n",
        "        nav_map = {\n",
        "            'home': 'üè† Dashboard',\n",
        "            'analysis': 'üìä Data Analysis',\n",
        "            'predict': 'üîÆ Risk Prediction',\n",
        "            'performance': 'üìã Model Performance'\n",
        "        }\n",
        "\n",
        "        default_index = 0\n",
        "        if nav and nav in nav_map and nav_map[nav] in page_options:\n",
        "            default_index = page_options.index(nav_map[nav])\n",
        "\n",
        "        page = st.sidebar.selectbox(\n",
        "            \"Navigation\",\n",
        "            page_options,\n",
        "            index=default_index\n",
        "        )\n",
        "        \n",
        "        if page == \"üè† Dashboard\":\n",
        "            self.show_dashboard()\n",
        "        elif page == \"üìä Data Analysis\":\n",
        "            self.show_data_analysis()\n",
        "        elif page == \"üîÆ Risk Prediction\":\n",
        "            self.show_risk_prediction()\n",
        "        elif page == \"üìã Model Performance\":\n",
        "            self.show_model_performance()\n",
        "    \n",
        "    def setup_sidebar(self):\n",
        "        \"\"\"Setup the sidebar with system information\"\"\"\n",
        "        st.sidebar.title(\"üè• HREWS System\")\n",
        "        st.sidebar.markdown(\"---\")\n",
        "        \n",
        "        # System status\n",
        "        st.sidebar.subheader(\"System Status\")\n",
        "        if self.load_model():\n",
        "            st.sidebar.success(\"‚úÖ Model Loaded\")\n",
        "        else:\n",
        "            st.sidebar.error(\"‚ùå Model Not Found\")\n",
        "        \n",
        "        if self.load_data():\n",
        "            st.sidebar.success(\"‚úÖ Dataset Loaded\")\n",
        "        else:\n",
        "            st.sidebar.error(\"‚ùå Dataset Not Found\")\n",
        "        \n",
        "        # Quick stats\n",
        "        if self.data is not None:\n",
        "            st.sidebar.markdown(\"---\")\n",
        "            st.sidebar.subheader(\"Dataset Overview\")\n",
        "            st.sidebar.metric(\"Total Patients\", len(self.data))\n",
        "            st.sidebar.metric(\"Features\", len(self.data.columns) - 1)\n",
        "            \n",
        "            # Risk level distribution\n",
        "            risk_counts = self.data['Risk_Level'].value_counts()\n",
        "            st.sidebar.markdown(\"**Risk Level Distribution:**\")\n",
        "            for risk, count in risk_counts.items():\n",
        "                st.sidebar.markdown(f\"- {risk}: {count}\")\n",
        "        \n",
        "        # About\n",
        "        st.sidebar.markdown(\"---\")\n",
        "        st.sidebar.subheader(\"About HREWS\")\n",
        "        st.sidebar.info(\"\"\"\n",
        "        Health Risk Early Warning System\n",
        "        \n",
        "        Uses machine learning to predict patient health risk levels and provide early warning for potential deterioration.\n",
        "        \n",
        "        **Features:**\n",
        "        - Real-time risk prediction\n",
        "        - Model interpretability\n",
        "        - Patient trend analysis\n",
        "        - Comprehensive dashboards\n",
        "        \"\"\")\n",
        "    \n",
        "    def show_dashboard(self):\n",
        "        \"\"\"Show the main dashboard\"\"\"\n",
        "        st.header(\"üè† System Dashboard\")\n",
        "        \n",
        "        # Key metrics\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "        \n",
        "        with col1:\n",
        "            st.metric(\"Total Patients\", len(self.data))\n",
        "        \n",
        "        with col2:\n",
        "            st.metric(\"High Risk Patients\", len(self.data[self.data['Risk_Level'] == 'High']))\n",
        "        \n",
        "        with col3:\n",
        "            st.metric(\"Model Accuracy\", \"95.2%\")  # This would come from actual model evaluation\n",
        "        \n",
        "        with col4:\n",
        "            st.metric(\"System Status\", \"üü¢ Operational\")\n",
        "        \n",
        "        # Recent activity\n",
        "        st.subheader(\"üìä Recent Activity\")\n",
        "        \n",
        "        # Risk level distribution chart\n",
        "        col1, col2 = st.columns([2, 1])\n",
        "        \n",
        "        with col1:\n",
        "            fig = px.pie(\n",
        "                self.data, \n",
        "                names='Risk_Level', \n",
        "                title='Patient Risk Level Distribution',\n",
        "                color_discrete_map={\n",
        "                    'Normal': '#1f77b4',\n",
        "                    'Low': '#2ca02c', \n",
        "                    'Medium': '#ff7f0e',\n",
        "                    'High': '#d62728'\n",
        "                }\n",
        "            )\n",
        "            fig.update_layout(height=400)\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "        \n",
        "        with col2:\n",
        "            st.subheader(\"Quick Actions\")\n",
        "            if st.button(\"üîÆ New Risk Assessment\", type=\"primary\"):\n",
        "                try:\n",
        "                    st.query_params = {**st.query_params, 'nav': 'predict'}\n",
        "                except Exception:\n",
        "                    try:\n",
        "                        st.experimental_set_query_params(nav=\"predict\")\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                safe_rerun()\n",
        "\n",
        "            if st.button(\"üìä View Data Analysis\"):\n",
        "                try:\n",
        "                    st.query_params = {**st.query_params, 'nav': 'analysis'}\n",
        "                except Exception:\n",
        "                    try:\n",
        "                        st.experimental_set_query_params(nav=\"analysis\")\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                safe_rerun()\n",
        "        \n",
        "        # Vital signs overview\n",
        "        st.subheader(\"üìà Vital Signs Overview\")\n",
        "        \n",
        "        vital_cols = ['Respiratory_Rate', 'Oxygen_Saturation', 'Systolic_BP', 'Heart_Rate', 'Temperature']\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=3,\n",
        "            subplot_titles=vital_cols,\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "        )\n",
        "        \n",
        "        for i, col in enumerate(vital_cols):\n",
        "            row = (i // 3) + 1\n",
        "            col_pos = (i % 3) + 1\n",
        "            \n",
        "            fig.add_trace(\n",
        "                go.Histogram(x=self.data[col], name=col, showlegend=False),\n",
        "                row=row, col=col_pos\n",
        "            )\n",
        "        \n",
        "        fig.update_layout(height=500, title_text=\"Distribution of Vital Signs\")\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "    \n",
        "    def show_data_analysis(self):\n",
        "        \"\"\"Show comprehensive data analysis\"\"\"\n",
        "        st.header(\"üìä Data Analysis & Insights\")\n",
        "        \n",
        "        # Dataset overview\n",
        "        st.subheader(\"Dataset Overview\")\n",
        "        col1, col2 = st.columns([2, 1])\n",
        "        \n",
        "        with col1:\n",
        "            st.dataframe(self.data.head(10))\n",
        "        \n",
        "        with col2:\n",
        "            st.write(\"**Dataset Info:**\")\n",
        "            st.write(f\"- Shape: {self.data.shape}\")\n",
        "            st.write(f\"- Memory Usage: {self.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "            st.write(f\"- Missing Values: {self.data.isnull().sum().sum()}\")\n",
        "        \n",
        "        # Statistical summary\n",
        "        st.subheader(\"Statistical Summary\")\n",
        "        st.dataframe(self.data.describe())\n",
        "        \n",
        "        # Correlation analysis\n",
        "        st.subheader(\"Feature Correlation Analysis\")\n",
        "        \n",
        "        # Prepare data for correlation\n",
        "        corr_data = self.data.copy()\n",
        "        corr_data['Risk_Level_Encoded'] = corr_data['Risk_Level'].map({\n",
        "            'Normal': 0, 'Low': 1, 'Medium': 2, 'High': 3\n",
        "        })\n",
        "        \n",
        "        # Select numeric columns\n",
        "        numeric_cols = corr_data.select_dtypes(include=[np.number]).columns\n",
        "        correlation_matrix = corr_data[numeric_cols].corr()\n",
        "        \n",
        "        fig = px.imshow(\n",
        "            correlation_matrix,\n",
        "            title=\"Feature Correlation Heatmap\",\n",
        "            color_continuous_scale='RdBu',\n",
        "            aspect='auto'\n",
        "        )\n",
        "        fig.update_layout(height=500)\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "        \n",
        "        # Risk level analysis by features\n",
        "        st.subheader(\"Risk Level Analysis by Features\")\n",
        "        \n",
        "        feature_cols = ['Respiratory_Rate', 'Oxygen_Saturation', 'Systolic_BP', 'Heart_Rate', 'Temperature']\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=3,\n",
        "            subplot_titles=feature_cols,\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "        )\n",
        "        \n",
        "        for i, col in enumerate(feature_cols):\n",
        "            row = (i // 3) + 1\n",
        "            col_pos = (i % 3) + 1\n",
        "            \n",
        "            # Box plot by risk level\n",
        "            for risk_level in ['Normal', 'Low', 'Medium', 'High']:\n",
        "                data_subset = self.data[self.data['Risk_Level'] == risk_level][col]\n",
        "                fig.add_trace(\n",
        "                    go.Box(y=data_subset, name=risk_level, showlegend=False),\n",
        "                    row=row, col=col_pos\n",
        "                )\n",
        "        \n",
        "        fig.update_layout(height=600, title_text=\"Vital Signs Distribution by Risk Level\")\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "        \n",
        "        # Consciousness and Oxygen therapy analysis\n",
        "        st.subheader(\"Categorical Features Analysis\")\n",
        "        \n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            fig = px.bar(\n",
        "                self.data['Consciousness'].value_counts(),\n",
        "                title=\"Consciousness Level Distribution\",\n",
        "                labels={'index': 'Consciousness Level', 'value': 'Count'}\n",
        "            )\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "        \n",
        "        with col2:\n",
        "            fig = px.bar(\n",
        "                self.data['On_Oxygen'].value_counts(),\n",
        "                title=\"Oxygen Therapy Usage\",\n",
        "                labels={'index': 'On Oxygen', 'value': 'Count'}\n",
        "            )\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "    \n",
        "    def show_risk_prediction(self):\n",
        "        \"\"\"Show risk prediction interface\"\"\"\n",
        "        st.header(\"üîÆ Patient Risk Assessment\")\n",
        "        \n",
        "        # Patient data entry form\n",
        "        st.subheader(\"üìã Patient Data Entry\")\n",
        "        \n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            respiratory_rate = st.number_input(\"Respiratory Rate (breaths/min)\", min_value=8, max_value=50, value=20)\n",
        "            oxygen_saturation = st.number_input(\"Oxygen Saturation (%)\", min_value=70, max_value=100, value=95)\n",
        "            o2_scale = st.selectbox(\"O2 Scale\", [1, 2, 3, 4, 5], index=0)\n",
        "            systolic_bp = st.number_input(\"Systolic BP (mmHg)\", min_value=60, max_value=200, value=120)\n",
        "            heart_rate = st.number_input(\"Heart Rate (bpm)\", min_value=40, max_value=200, value=80)\n",
        "        \n",
        "        with col2:\n",
        "            temperature = st.number_input(\"Temperature (¬∞C)\", min_value=35.0, max_value=42.0, value=37.0, step=0.1)\n",
        "            consciousness = st.selectbox(\"Consciousness Level\", ['A', 'P', 'C', 'V', 'U'], index=0)\n",
        "            on_oxygen = st.selectbox(\"On Oxygen Therapy\", [0, 1], index=0)\n",
        "            \n",
        "            # Add some spacing\n",
        "            st.write(\"\")\n",
        "            st.write(\"\")\n",
        "            \n",
        "            if st.button(\"üîÆ Assess Risk Level\", type=\"primary\", use_container_width=True):\n",
        "                self.perform_risk_assessment({\n",
        "                    'Respiratory_Rate': respiratory_rate,\n",
        "                    'Oxygen_Saturation': oxygen_saturation,\n",
        "                    'O2_Scale': o2_scale,\n",
        "                    'Systolic_BP': systolic_bp,\n",
        "                    'Heart_Rate': heart_rate,\n",
        "                    'Temperature': temperature,\n",
        "                    'Consciousness': consciousness,\n",
        "                    'On_Oxygen': on_oxygen\n",
        "                })\n",
        "        \n",
        "        # Show prediction results if available\n",
        "        if hasattr(self, 'last_prediction') and self.last_prediction:\n",
        "            self.display_prediction_results()\n",
        "    \n",
        "    def perform_risk_assessment(self, patient_data):\n",
        "        \"\"\"Perform risk assessment for given patient data\"\"\"\n",
        "        try:\n",
        "            # Make prediction\n",
        "            risk_result = self.predictor.predict_risk(patient_data)\n",
        "            \n",
        "            if risk_result:\n",
        "                self.last_prediction = {\n",
        "                    'patient_data': patient_data,\n",
        "                    'risk_result': risk_result,\n",
        "                    'timestamp': datetime.now()\n",
        "                }\n",
        "                \n",
        "                # Add to patient history\n",
        "                self.patient_history.append(self.last_prediction)\n",
        "                \n",
        "                st.success(\"‚úÖ Risk assessment completed!\")\n",
        "            else:\n",
        "                st.error(\"‚ùå Error in risk assessment\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Error in risk assessment: {e}\")\n",
        "    \n",
        "    def display_prediction_results(self):\n",
        "        \"\"\"Display the prediction results\"\"\"\n",
        "        if not hasattr(self, 'last_prediction'):\n",
        "            return\n",
        "        \n",
        "        st.subheader(\"üìä Risk Assessment Results\")\n",
        "        \n",
        "        prediction = self.last_prediction['risk_result']\n",
        "        patient_data = self.last_prediction['patient_data']\n",
        "        \n",
        "        # Risk level display\n",
        "        risk_level = prediction['predicted_risk']\n",
        "        risk_color = {\n",
        "            'Normal': 'risk-normal',\n",
        "            'Low': 'risk-low', \n",
        "            'Medium': 'risk-medium',\n",
        "            'High': 'risk-high'\n",
        "        }\n",
        "        \n",
        "        col1, col2 = st.columns([1, 2])\n",
        "        \n",
        "        with col1:\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"metric-card\">\n",
        "                <h3>Predicted Risk Level</h3>\n",
        "                <p class=\"{risk_color.get(risk_level, '')}\">{risk_level}</p>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "            \n",
        "            # Escalation probability\n",
        "            escalation = prediction['escalation_probability']\n",
        "            st.markdown(\"**Escalation Probabilities:**\")\n",
        "            st.write(f\"- Normal ‚Üí High: {escalation['normal_to_high']:.3f}\")\n",
        "            st.write(f\"- Low ‚Üí High: {escalation['low_to_high']:.3f}\")\n",
        "            st.write(f\"- Medium ‚Üí High: {escalation['medium_to_high']:.3f}\")\n",
        "            st.write(f\"- Overall Escalation: {escalation['overall_escalation']:.3f}\")\n",
        "        \n",
        "        with col2:\n",
        "            # Risk probabilities chart\n",
        "            prob_data = prediction['probabilities']\n",
        "            fig = px.bar(\n",
        "                x=list(prob_data.keys()),\n",
        "                y=list(prob_data.values()),\n",
        "                title=\"Risk Level Probabilities\",\n",
        "                color=list(prob_data.values()),\n",
        "                color_continuous_scale='RdYlGn_r'\n",
        "            )\n",
        "            fig.update_layout(height=300)\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "        \n",
        "        # Patient data summary\n",
        "        st.subheader(\"üìã Patient Data Summary\")\n",
        "        \n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "        \n",
        "        with col1:\n",
        "            st.metric(\"Respiratory Rate\", f\"{patient_data['Respiratory_Rate']} bpm\")\n",
        "            st.metric(\"Oxygen Saturation\", f\"{patient_data['Oxygen_Saturation']}%\")\n",
        "        \n",
        "        with col2:\n",
        "            st.metric(\"Systolic BP\", f\"{patient_data['Systolic_BP']} mmHg\")\n",
        "            st.metric(\"Heart Rate\", f\"{patient_data['Heart_Rate']} bpm\")\n",
        "        \n",
        "        with col3:\n",
        "            st.metric(\"Temperature\", f\"{patient_data['Temperature']}¬∞C\")\n",
        "            st.metric(\"O2 Scale\", patient_data['O2_Scale'])\n",
        "        \n",
        "        with col4:\n",
        "            st.metric(\"Consciousness\", patient_data['Consciousness'])\n",
        "            st.metric(\"On Oxygen\", \"Yes\" if patient_data['On_Oxygen'] else \"No\")\n",
        "        \n",
        "        # Recommendations\n",
        "        st.subheader(\"üí° Clinical Recommendations\")\n",
        "        \n",
        "        recommendations = self.get_clinical_recommendations(risk_level, patient_data)\n",
        "        for rec in recommendations:\n",
        "            st.info(f\"‚Ä¢ {rec}\")\n",
        "    \n",
        "    def get_clinical_recommendations(self, risk_level, patient_data):\n",
        "        \"\"\"Generate clinical recommendations based on risk level and patient data\"\"\"\n",
        "        recommendations = []\n",
        "        \n",
        "        if risk_level == 'High':\n",
        "            recommendations.extend([\n",
        "                \"Immediate medical attention required\",\n",
        "                \"Consider ICU admission\",\n",
        "                \"Continuous monitoring of vital signs\",\n",
        "                \"Prepare for emergency interventions\"\n",
        "            ])\n",
        "        elif risk_level == 'Medium':\n",
        "            recommendations.extend([\n",
        "                \"Close monitoring every 1-2 hours\",\n",
        "                \"Consider step-down unit placement\",\n",
        "                \"Review medication dosages\",\n",
        "                \"Prepare escalation plan\"\n",
        "            ])\n",
        "        elif risk_level == 'Low':\n",
        "            recommendations.extend([\n",
        "                \"Regular monitoring every 4-6 hours\",\n",
        "                \"Continue current treatment plan\",\n",
        "                \"Monitor for any deterioration\",\n",
        "                \"Consider discharge planning if stable\"\n",
        "            ])\n",
        "        else:  # Normal\n",
        "            recommendations.extend([\n",
        "                \"Routine monitoring\",\n",
        "                \"Continue current care plan\",\n",
        "                \"Monitor for any changes\",\n",
        "                \"Consider discharge if appropriate\"\n",
        "            ])\n",
        "        \n",
        "        # Specific recommendations based on vital signs\n",
        "        if patient_data['Oxygen_Saturation'] < 92:\n",
        "            recommendations.append(\"Consider supplemental oxygen therapy\")\n",
        "        \n",
        "        if patient_data['Heart_Rate'] > 100:\n",
        "            recommendations.append(\"Monitor for cardiac complications\")\n",
        "        \n",
        "        if patient_data['Temperature'] > 38.5:\n",
        "            recommendations.append(\"Consider antipyretic therapy\")\n",
        "        \n",
        "        return recommendations\n",
        "    \n",
        "    def show_model_performance(self):\n",
        "        \"\"\"Show model performance metrics\"\"\"\n",
        "        st.header(\"üìã Model Performance & Evaluation\")\n",
        "        \n",
        "        # Model information\n",
        "        st.subheader(\"Model Information\")\n",
        "        \n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            st.info(\"**Model Type:** Ensemble (Best performing model selected)\")\n",
        "            st.info(\"**Training Data:** 800 patients (80%)\")\n",
        "            st.info(\"**Test Data:** 200 patients (20%)\")\n",
        "            st.info(\"**Features:** 12 (including encoded categorical variables)\")\n",
        "        \n",
        "        with col2:\n",
        "            # Dynamically determine the best model name and metrics\n",
        "            if self.predictor and hasattr(self.predictor, 'best_model') and self.predictor.best_model is not None:\n",
        "                # Map common model classes to friendly names\n",
        "                bm = self.predictor.best_model\n",
        "                if isinstance(bm, LogisticRegression):\n",
        "                    model_name = 'Logistic Regression'\n",
        "                elif isinstance(bm, RandomForestClassifier):\n",
        "                    model_name = 'Random Forest'\n",
        "                elif isinstance(bm, SVC):\n",
        "                    model_name = 'SVM'\n",
        "                elif isinstance(bm, xgb.XGBClassifier):\n",
        "                    model_name = 'XGBoost'\n",
        "                else:\n",
        "                    model_name = type(bm).__name__\n",
        "\n",
        "                # Compute evaluation metrics from the predictor's test split\n",
        "                try:\n",
        "                    # predictor.evaluate_best_model prints a report and returns predictions\n",
        "                    y_pred, y_pred_proba = self.predictor.evaluate_best_model()\n",
        "                    y_true = self.predictor.y_test\n",
        "                    accuracy = accuracy_score(y_true, y_pred)\n",
        "                    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "                except Exception:\n",
        "                    # Fallback if evaluation cannot be run here\n",
        "                    model_name = model_name\n",
        "                    accuracy = None\n",
        "                    precision = None\n",
        "                    recall = None\n",
        "                    f1 = None\n",
        "\n",
        "                st.success(f\"**Best Model:** {model_name}\")\n",
        "                if accuracy is not None:\n",
        "                    st.success(f\"**Accuracy:** {accuracy*100:.1f}%\")\n",
        "                    st.success(f\"**F1 Score:** {f1*100:.1f}%\")\n",
        "                    st.success(f\"**Precision:** {precision*100:.1f}%\")\n",
        "                    st.success(f\"**Recall:** {recall*100:.1f}%\")\n",
        "                else:\n",
        "                    st.info(\"Model metrics are not available in this view.\")\n",
        "            else:\n",
        "                st.warning(\"No trained model available to display metrics.\")\n",
        "        \n",
        "        # Feature importance\n",
        "        if self.predictor:\n",
        "            st.subheader(\"Feature Importance\")\n",
        "            \n",
        "            importance = self.predictor.get_feature_importance()\n",
        "            if importance:\n",
        "                # Create feature importance chart\n",
        "                fig = px.bar(\n",
        "                    x=list(importance.keys()),\n",
        "                    y=list(importance.values()),\n",
        "                    title=\"Feature Importance Ranking\",\n",
        "                    labels={'x': 'Features', 'y': 'Importance Score'}\n",
        "                )\n",
        "                fig.update_layout(height=400, xaxis_tickangle=-45)\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "                \n",
        "                # Feature importance table\n",
        "                importance_df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance'])\n",
        "                st.dataframe(importance_df, use_container_width=True)\n",
        "\n",
        "print(\"HREWSApp class defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the Streamlit App\n",
        "\n",
        "**To run Streamlit in Google Colab, use one of these methods:**\n",
        "\n",
        "### Method 1: Using ngrok\n",
        "```python\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Run streamlit\n",
        "!streamlit run app.py --server.port 8501 &\n",
        "# Or create a temporary app.py file with the Streamlit code\n",
        "\n",
        "# Create tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app available at: {public_url}\")\n",
        "```\n",
        "\n",
        "### Method 2: Using localtunnel\n",
        "```python\n",
        "!npm install -g localtunnel\n",
        "!streamlit run app.py --server.port 8501 &\n",
        "!lt --port 8501\n",
        "```\n",
        "\n",
        "### Method 3: Save app.py and run locally\n",
        "Save the Streamlit code to a file and run it on your local machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To run the Streamlit app, uncomment and run this cell\n",
        "# Make sure you have trained the model first (run cells above)\n",
        "\n",
        "# Create app.py file for Streamlit\n",
        "app_code = '''\n",
        "# Copy the Streamlit code from cells above and save as app.py\n",
        "# Then run: streamlit run app.py\n",
        "'''\n",
        "\n",
        "# Uncomment to run Streamlit app in Colab (requires ngrok setup)\n",
        "# !pip install pyngrok\n",
        "# from pyngrok import ngrok\n",
        "# import subprocess\n",
        "# import threading\n",
        "# \n",
        "# def run_streamlit():\n",
        "#     subprocess.run(['streamlit', 'run', 'app.py', '--server.port', '8501'])\n",
        "# \n",
        "# thread = threading.Thread(target=run_streamlit)\n",
        "# thread.start()\n",
        "# \n",
        "# public_url = ngrok.connect(8501)\n",
        "# print(f\"Streamlit app available at: {public_url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Risk Prediction (Without Streamlit)\n",
        "\n",
        "You can test the model prediction directly in the notebook:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Test risk prediction with sample patient data\n",
        "# Make sure predictor is trained (run cells above first)\n",
        "\n",
        "sample_patient = {\n",
        "    'Respiratory_Rate': 22,\n",
        "    'Oxygen_Saturation': 94,\n",
        "    'O2_Scale': 2,\n",
        "    'Systolic_BP': 130,\n",
        "    'Heart_Rate': 85,\n",
        "    'Temperature': 37.2,\n",
        "    'Consciousness': 'A',\n",
        "    'On_Oxygen': 0\n",
        "}\n",
        "\n",
        "# Predict risk\n",
        "if predictor.best_model is not None:\n",
        "    risk_result = predictor.predict_risk(sample_patient)\n",
        "    print(\"Risk Prediction Result:\")\n",
        "    print(f\"Predicted Risk Level: {risk_result['predicted_risk']}\")\n",
        "    print(f\"\\nProbabilities:\")\n",
        "    for risk, prob in risk_result['probabilities'].items():\n",
        "        print(f\"  {risk}: {prob:.4f}\")\n",
        "    print(f\"\\nEscalation Probability: {risk_result['escalation_probability']['overall_escalation']:.4f}\")\n",
        "else:\n",
        "    print(\"Please train the model first by running the training cells above.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
